{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1256d3b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import glob\n",
    "import math\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import sys\n",
    "import shutil # Added for file copying\n",
    "\n",
    "# --- Check and Install required packages ---\n",
    "try:\n",
    "    from pesq import pesq\n",
    "    from pystoi import stoi\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Installing required packages: pesq and pystoi...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pesq\", \"pystoi\"])\n",
    "    from pesq import pesq\n",
    "    from pystoi import stoi\n",
    "    print(\"Installation complete. Continuing script execution.\")\n",
    "\n",
    "# --- Configuration for Causal VQ-Codec ---\n",
    "SR = 16000\n",
    "CHANNELS = 1\n",
    "LATENT_DIM = 64\n",
    "BLOCKS = 4\n",
    "HEADS = 4\n",
    "KERNEL_SIZE = 3\n",
    "STRIDES = [2, 2, 4, 2] \n",
    "DOWN_FACTOR = np.prod(STRIDES)\n",
    "HOP_SIZE_MS = 20\n",
    "CHUNK_DURATION = 0.04 \n",
    "WINDOW_SAMPLES = int(CHUNK_DURATION * SR) # 640 samples\n",
    "HOP_SAMPLES = int(HOP_SIZE_MS * SR / 1000)\n",
    "NUM_CODEBOOKS = 2\n",
    "CODEBOOK_SIZE = 512\n",
    "BITRATE_TARGET = (SR / DOWN_FACTOR * math.log2(CODEBOOK_SIZE) * NUM_CODEBOOKS) / 1000 \n",
    "\n",
    "# Training Hyperparameters\n",
    "SPECTRAL_LOSS_WEIGHT = 1.0\n",
    "VQ_LOSS_WEIGHT = 0.1\n",
    "COMMITMENT_COST = 1.0\n",
    "TRANSFORMER_BLOCKS = 3\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "MONITOR_FREQUENCY = 10\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = './TinyCodec_Training'\n",
    "CHECKPOINT_PATH = f'{BASE_DIR}/checkpoint_full.pth'\n",
    "BEST_MODEL_PATH = f'{BASE_DIR}/best_model.pth'\n",
    "DATA_DIR = f'{BASE_DIR}/dataset'\n",
    "\n",
    "# --- GOOGLE DRIVE PATH (New Addition) ---\n",
    "# NOTE: This path assumes Google Drive is mounted at /content/drive/MyDrive/\n",
    "GDRIVE_SAVE_DIR = '/content/drive/MyDrive/TinyCodec_Checkpoints'\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "GRADIENT_CLIPPING_NORM = 1.0\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(GDRIVE_SAVE_DIR, exist_ok=True) # Create GDrive save directory\n",
    "\n",
    "print(f\"Target Bitrate (Achieved): {BITRATE_TARGET:.2f} kbps\")\n",
    "print(f\"Target Hop Latency: {HOP_SIZE_MS} ms\")\n",
    "print(f\"Latent Frame Rate: {SR / DOWN_FACTOR} Hz\")\n",
    "print(f\"Window Samples: {WINDOW_SAMPLES}\")\n",
    "\n",
    "# --- Dataset Download Function ---\n",
    "def download_dataset():\n",
    "    \"\"\"Downloads LJSpeech dataset automatically.\"\"\"\n",
    "    dataset_url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n",
    "    dataset_path = os.path.join(DATA_DIR, \"LJSpeech-1.1.tar.bz2\")\n",
    "    extract_path = os.path.join(DATA_DIR, \"LJSpeech-1.1\")\n",
    "    \n",
    "    if os.path.exists(os.path.join(extract_path, \"wavs\")):\n",
    "        print(\"Dataset already exists. Skipping download.\")\n",
    "        return os.path.join(extract_path, \"wavs\")\n",
    "    \n",
    "    def download_with_progress(url, filepath):\n",
    "        def download_hook(block_num, block_size, total_size):\n",
    "            downloaded = block_num * block_size\n",
    "            percent = min(downloaded * 100 / total_size, 100)\n",
    "            mb_downloaded = downloaded / 1024 / 1024\n",
    "            mb_total = total_size / 1024 / 1024\n",
    "            print(f\"Downloading: {percent:.1f}% ({mb_downloaded:.1f}/{mb_total:.1f} MB)\", end='\\r')\n",
    "        \n",
    "        print(f\"Downloading LJSpeech dataset from {url}\")\n",
    "        urllib.request.urlretrieve(url, filepath, reporthook=download_hook)\n",
    "        print(\"\\nDownload complete!\")\n",
    "    \n",
    "    if not os.path.exists(dataset_path):\n",
    "        download_with_progress(dataset_url, dataset_path)\n",
    "    \n",
    "    print(\"Extracting dataset...\")\n",
    "    with tarfile.open(dataset_path, 'r:bz2') as tar:\n",
    "        tar.extractall(DATA_DIR)\n",
    "    print(\"Extraction complete!\")\n",
    "    \n",
    "    os.remove(dataset_path)\n",
    "    \n",
    "    return os.path.join(extract_path, \"wavs\")\n",
    "\n",
    "def download_mini_dataset():\n",
    "    \"\"\"Downloads a smaller dataset for quick testing.\"\"\"\n",
    "    print(\"Creating mini speech dataset for testing...\")\n",
    "    os.makedirs(os.path.join(DATA_DIR, \"mini_wavs\"), exist_ok=True)\n",
    "    \n",
    "    for i in range(100):\n",
    "        duration = 2.0\n",
    "        t = torch.linspace(0, duration, int(SR * duration))\n",
    "        \n",
    "        frequency = 200 + np.random.randint(-50, 50)\n",
    "        signal = torch.sin(2 * np.pi * frequency * t)\n",
    "        signal += 0.5 * torch.sin(2 * np.pi * frequency * 2 * t)\n",
    "        signal += 0.3 * torch.sin(2 * np.pi * frequency * 3 * t)\n",
    "        \n",
    "        envelope = torch.exp(-t * 0.5) * (1 + 0.5 * torch.sin(2 * np.pi * 3 * t))\n",
    "        signal = signal * envelope\n",
    "        \n",
    "        signal += 0.01 * torch.randn_like(signal)\n",
    "        signal = signal / torch.max(torch.abs(signal))\n",
    "        \n",
    "        filepath = os.path.join(DATA_DIR, \"mini_wavs\", f\"audio_{i:04d}.wav\")\n",
    "        torchaudio.save(filepath, signal.unsqueeze(0), SR)\n",
    "    \n",
    "    print(f\"Created 100 sample audio files in {os.path.join(DATA_DIR, 'mini_wavs')}\")\n",
    "    return os.path.join(DATA_DIR, \"mini_wavs\")\n",
    "\n",
    "# --- Model Components ---\n",
    "class CausalConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        self.padding_amount = kernel_size - 1\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding=0)\n",
    "        self.norm = nn.GroupNorm(1, out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, (self.padding_amount, 0), mode='constant', value=0)\n",
    "        x = self.relu(self.norm(self.conv(x)))\n",
    "        return x\n",
    "\n",
    "class CausalTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(dim, heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, T = x.shape\n",
    "        x_attn = x.transpose(1, 2)\n",
    "        \n",
    "        attn_mask = torch.triu(torch.ones(T, T, dtype=torch.bool, device=x.device), diagonal=1)\n",
    "        \n",
    "        attn_output, _ = self.attn(x_attn, x_attn, x_attn, attn_mask=attn_mask, is_causal=False)\n",
    "        x_attn = self.norm1(x_attn + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(x_attn)\n",
    "        x_attn = self.norm2(x_attn + ffn_output)\n",
    "        \n",
    "        return x_attn.transpose(1, 2)\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost=COMMITMENT_COST):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.commitment_cost = commitment_cost\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / self.num_embeddings, 1.0 / self.num_embeddings)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        flat_input = inputs.transpose(1, 2).contiguous().view(-1, self.embedding_dim)\n",
    "        \n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    "                      + torch.sum(self.embedding.weight**2, dim=1)\n",
    "                      - 2 * torch.matmul(flat_input, self.embedding.weight.t()))\n",
    "            \n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        \n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        quantized = torch.matmul(encodings, self.embedding.weight).view(inputs.shape[0], inputs.shape[2], -1).transpose(1, 2)\n",
    "        \n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs) \n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach()) \n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        \n",
    "        return quantized, loss, encoding_indices\n",
    "\n",
    "class TinyTransformerCodec(nn.Module):\n",
    "    def __init__(self, latent_dim=LATENT_DIM, blocks=BLOCKS, heads=HEADS, sr=SR):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sr = sr\n",
    "        self.downsampling_factor = DOWN_FACTOR\n",
    "        self.num_codebooks = NUM_CODEBOOKS\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_convs = nn.ModuleList()\n",
    "        in_c = CHANNELS\n",
    "        encoder_channels = []\n",
    "        \n",
    "        # Define encoder channel progression\n",
    "        for i in range(blocks):\n",
    "            out_c = min(latent_dim, 8 * (2**i)) # 8, 16, 32, 64\n",
    "            encoder_channels.append(out_c)\n",
    "            stride = STRIDES[i]\n",
    "            self.encoder_convs.append(\n",
    "                CausalConvBlock(in_c, out_c, KERNEL_SIZE, stride)\n",
    "            )\n",
    "            in_c = out_c\n",
    "        \n",
    "        self.pre_quant = CausalConvBlock(in_c, LATENT_DIM * NUM_CODEBOOKS, KERNEL_SIZE, 1)\n",
    "\n",
    "        # Vector Quantization\n",
    "        self.quantizers = nn.ModuleList([\n",
    "            VectorQuantizer(CODEBOOK_SIZE, LATENT_DIM, commitment_cost=COMMITMENT_COST)\n",
    "            for _ in range(NUM_CODEBOOKS)\n",
    "        ])\n",
    "\n",
    "        # Transformer\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            CausalTransformerBlock(latent_dim * NUM_CODEBOOKS, heads)\n",
    "            for _ in range(TRANSFORMER_BLOCKS)\n",
    "        ])\n",
    "        self.post_transformer = nn.Conv1d(latent_dim * NUM_CODEBOOKS, latent_dim * NUM_CODEBOOKS, 1)\n",
    "\n",
    "        # Decoder - process in reverse order\n",
    "        self.decoder_tconvs = nn.ModuleList()\n",
    "        self.skip_convs = nn.ModuleList()\n",
    "        \n",
    "        in_c = latent_dim * NUM_CODEBOOKS\n",
    "        decoder_channels = []\n",
    "        \n",
    "        # Build decoder layers in reverse order of encoder\n",
    "        for i in range(blocks):\n",
    "            idx = blocks - 1 - i # Reverse index\n",
    "            stride = STRIDES[idx]\n",
    "            \n",
    "            # Determine output channels\n",
    "            if idx > 0:\n",
    "                out_c = encoder_channels[idx - 1]\n",
    "            else:\n",
    "                out_c = 16 # Base channel count before final layer\n",
    "            \n",
    "            decoder_channels.append(out_c)\n",
    "            \n",
    "            # Transposed convolution for upsampling\n",
    "            self.decoder_tconvs.append(\n",
    "                nn.ConvTranspose1d(in_c, out_c, KERNEL_SIZE, stride, padding=KERNEL_SIZE//2)\n",
    "            )\n",
    "            \n",
    "            # Skip connections (except for last decoder layer)\n",
    "            if idx > 0:\n",
    "                skip_in_channels = encoder_channels[idx - 1]\n",
    "                # Skip conv: concatenated channels -> output channels\n",
    "                self.skip_convs.append(\n",
    "                    nn.Conv1d(out_c + skip_in_channels, out_c, kernel_size=1)\n",
    "                )\n",
    "            \n",
    "            in_c = out_c\n",
    "        \n",
    "        # Final output layer\n",
    "        self.post_decoder_final = nn.Conv1d(in_c, CHANNELS, 1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = x.view(x.size(0), CHANNELS, -1)\n",
    "        input_length = x.shape[-1]\n",
    "        \n",
    "        encoder_outputs = []\n",
    "        \n",
    "        # Encoder\n",
    "        for layer in self.encoder_convs:\n",
    "            x = layer(x)\n",
    "            encoder_outputs.append(x)\n",
    "        \n",
    "        # Pre-quantization\n",
    "        z_e = self.pre_quant(x)\n",
    "        \n",
    "        # Vector Quantization\n",
    "        z_q_list = []\n",
    "        vq_loss_total = 0.0\n",
    "        indices_list = []\n",
    "        \n",
    "        z_e_split = z_e.chunk(self.num_codebooks, dim=1)\n",
    "        \n",
    "        for i in range(self.num_codebooks):\n",
    "            z_q, vq_loss, indices = self.quantizers[i](z_e_split[i])\n",
    "            z_q_list.append(z_q)\n",
    "            vq_loss_total += vq_loss\n",
    "            indices_list.append(indices)\n",
    "        \n",
    "        z_q_concat = torch.cat(z_q_list, dim=1)\n",
    "        \n",
    "        # Transformer\n",
    "        codes = self.transformer(z_q_concat)\n",
    "        codes = self.post_transformer(codes)\n",
    "        \n",
    "        return codes, vq_loss_total, input_length, indices_list, encoder_outputs\n",
    "\n",
    "    def decode(self, codes, input_length=None, encoder_outputs=None):\n",
    "        x = codes\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        for i, tconv in enumerate(self.decoder_tconvs):\n",
    "            x = F.relu(tconv(x))\n",
    "            \n",
    "            # Apply skip connection if available\n",
    "            if encoder_outputs and i < len(self.skip_convs):\n",
    "                # Map decoder layer to encoder layer for skip connection\n",
    "                # Decoder layer i corresponds to encoder layer (blocks - 2 - i)\n",
    "                encoder_idx = len(self.encoder_convs) - 2 - i\n",
    "                \n",
    "                if 0 <= encoder_idx < len(encoder_outputs):\n",
    "                    skip_features = encoder_outputs[encoder_idx]\n",
    "                    \n",
    "                    # Match temporal dimensions\n",
    "                    min_len = min(skip_features.shape[-1], x.shape[-1])\n",
    "                    skip_features = skip_features[..., :min_len]\n",
    "                    x_trim = x[..., :min_len]\n",
    "                    \n",
    "                    # Concatenate and apply skip conv\n",
    "                    x_cat = torch.cat([x_trim, skip_features], dim=1)\n",
    "                    x_processed = self.skip_convs[i](x_cat)\n",
    "                    \n",
    "                    # Restore original length if needed\n",
    "                    if x.shape[-1] > min_len:\n",
    "                        x = torch.cat([x_processed, x[..., min_len:]], dim=-1)\n",
    "                    else:\n",
    "                        x = x_processed\n",
    "        \n",
    "        # Final output\n",
    "        x = torch.tanh(self.post_decoder_final(x))\n",
    "        \n",
    "        # Match input length\n",
    "        if input_length is not None:\n",
    "            if x.shape[-1] > input_length:\n",
    "                x = x[..., :input_length]\n",
    "            elif x.shape[-1] < input_length:\n",
    "                x = F.pad(x, (0, input_length - x.shape[-1]))\n",
    "        \n",
    "        return x.view(x.size(0), CHANNELS, -1)\n",
    "\n",
    "# --- Loss Functions ---\n",
    "class SpectralLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Adjust FFT sizes to be appropriate for 640-sample windows\n",
    "        # Use smaller FFT sizes that fit within the window\n",
    "        self.stft_losses = nn.ModuleList()\n",
    "        \n",
    "        # FFT sizes must be smaller than window size (640 samples)\n",
    "        fft_sizes = [128, 256, 512]\n",
    "        hop_sizes = [32, 64, 128]\n",
    "        \n",
    "        for fft, hop in zip(fft_sizes, hop_sizes):\n",
    "            self.stft_losses.append(\n",
    "                torchaudio.transforms.Spectrogram(\n",
    "                    n_fft=fft, \n",
    "                    hop_length=hop, \n",
    "                    power=1,\n",
    "                    normalized=False,\n",
    "                    center=False # Don't pad the signal\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        loss = 0.0\n",
    "        for stft_fn in self.stft_losses:\n",
    "            X = stft_fn(x)\n",
    "            Y = stft_fn(y)\n",
    "            loss += F.l1_loss(X, Y)\n",
    "        return loss / len(self.stft_losses) # Average across different resolutions\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class AudioFileDataset(Dataset):\n",
    "    def __init__(self, data_dir, sr=SR, duration=CHUNK_DURATION, max_files=None):\n",
    "        self.sr = sr\n",
    "        self.chunk_len = int(duration * sr)\n",
    "        \n",
    "        self.file_list = glob.glob(os.path.join(data_dir, '*.wav'))\n",
    "        if max_files:\n",
    "            self.file_list = self.file_list[:max_files]\n",
    "            \n",
    "        if not self.file_list:\n",
    "            raise ValueError(f\"No .wav files found in {data_dir}\")\n",
    "            \n",
    "        print(f\"Found {len(self.file_list)} audio files\")\n",
    "        \n",
    "        self.audio_chunks = []\n",
    "        \n",
    "        for fpath in tqdm(self.file_list, desc=\"Loading audio files\"):\n",
    "            try:\n",
    "                wav, sample_rate = torchaudio.load(fpath)\n",
    "                \n",
    "                if sample_rate != self.sr:\n",
    "                    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.sr)\n",
    "                    wav = resampler(wav)\n",
    "                \n",
    "                wav = wav[0:1, :]\n",
    "                \n",
    "                for i in range(0, wav.shape[-1] - self.chunk_len + 1, self.chunk_len):\n",
    "                    chunk = wav[:, i:i+self.chunk_len]\n",
    "                    if chunk.shape[-1] == self.chunk_len:\n",
    "                        self.audio_chunks.append(chunk)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Skipping file {fpath}: {e}\")\n",
    "        \n",
    "        print(f\"Total audio chunks: {len(self.audio_chunks)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.audio_chunks[idx].clamp(-1.0, 1.0)\n",
    "\n",
    "# --- Monitoring Functions ---\n",
    "def run_quality_metrics(model, val_loader, device, sr=SR, max_batches=5):\n",
    "    model.eval()\n",
    "    \n",
    "    all_original = []\n",
    "    all_reconstructed = []\n",
    "    \n",
    "    val_iter = iter(val_loader)\n",
    "    for _ in range(min(max_batches, len(val_loader))):\n",
    "        try:\n",
    "            batch = next(val_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "        all_original.append(batch.cpu().numpy())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            audio = batch.to(device)\n",
    "            with torch.amp.autocast(device_type='cuda' if device.type == 'cuda' else 'cpu', dtype=torch.float16):\n",
    "                codes, _, input_length, _, encoder_outputs = model.encode(audio)\n",
    "                reconstructed_audio = model.decode(codes, input_length, encoder_outputs)\n",
    "        \n",
    "        all_reconstructed.append(reconstructed_audio.cpu().numpy())\n",
    "    \n",
    "    if not all_original:\n",
    "        model.train()\n",
    "        return 0.0, 0.0\n",
    "        \n",
    "    original_wavs = np.concatenate(all_original, axis=0)\n",
    "    reconstructed_wavs = np.concatenate(all_reconstructed, axis=0)\n",
    "    \n",
    "    pesq_scores = []\n",
    "    stoi_scores = []\n",
    "    \n",
    "    for i in range(min(5, original_wavs.shape[0])):\n",
    "        original = original_wavs[i, 0]\n",
    "        reconstructed = reconstructed_wavs[i, 0]\n",
    "        \n",
    "        min_len = min(len(original), len(reconstructed))\n",
    "        original, reconstructed = original[:min_len], reconstructed[:min_len]\n",
    "\n",
    "        try:\n",
    "            p = pesq(sr, original, reconstructed, 'wb')\n",
    "            pesq_scores.append(p)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        s = stoi(original, reconstructed, sr, extended=False)\n",
    "        stoi_scores.append(s)\n",
    "        \n",
    "    avg_pesq = np.mean(pesq_scores) if pesq_scores else 0.0\n",
    "    avg_stoi = np.mean(stoi_scores) if stoi_scores else 0.0\n",
    "    \n",
    "    model.train()\n",
    "    return avg_pesq, avg_stoi\n",
    "\n",
    "def monitor_codebook_usage(model, val_loader, device, max_batches=5):\n",
    "    model.eval()\n",
    "    \n",
    "    usage_counts = [torch.zeros(CODEBOOK_SIZE, device=device) for _ in range(NUM_CODEBOOKS)]\n",
    "    \n",
    "    val_iter = iter(val_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(min(max_batches, len(val_loader))):\n",
    "            try:\n",
    "                batch = next(val_iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            \n",
    "            audio = batch.to(device)\n",
    "            _, _, _, indices_list, _ = model.encode(audio)\n",
    "            \n",
    "            for cb_idx, indices in enumerate(indices_list):\n",
    "                unique_indices = torch.unique(indices)\n",
    "                usage_counts[cb_idx][unique_indices] += 1\n",
    "                \n",
    "    print(\"\\n--- Codebook Utilization Check ---\")\n",
    "    for cb_idx in range(NUM_CODEBOOKS):\n",
    "        used_codes = (usage_counts[cb_idx] > 0).sum().item()\n",
    "        utilization = (used_codes / CODEBOOK_SIZE) * 100\n",
    "        print(f\"Codebook {cb_idx}: {utilization:.1f}% ({used_codes}/{CODEBOOK_SIZE} codes)\")\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "def validate(model, val_loader, criterion, spectral_criterion, device):\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            audio = batch.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type='cuda' if device.type == 'cuda' else 'cpu', dtype=torch.float16):\n",
    "                codes, vq_loss, input_length, _, encoder_outputs = model.encode(audio)\n",
    "                reconstructed_audio = model.decode(codes, input_length, encoder_outputs)\n",
    "                \n",
    "                reconstruction_loss = criterion(reconstructed_audio, audio)\n",
    "                spectral_loss = spectral_criterion(reconstructed_audio, audio)\n",
    "                \n",
    "                loss = reconstruction_loss + SPECTRAL_LOSS_WEIGHT * spectral_loss + VQ_LOSS_WEIGHT * vq_loss\n",
    "                \n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    model.train()\n",
    "    return total_val_loss / len(val_loader) if len(val_loader) > 0 else 0.0\n",
    "\n",
    "# --- Main Training Function ---\n",
    "def train_model(use_mini_dataset=False):\n",
    "    \"\"\"Main training function with automatic dataset download.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TINY TRANSFORMER CODEC TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Dataset Preparation\n",
    "    print(\"\\n--- Step 1: Dataset Preparation ---\")\n",
    "    if use_mini_dataset:\n",
    "        audio_dir = download_mini_dataset()\n",
    "        max_files = None\n",
    "    else:\n",
    "        audio_dir = download_dataset()\n",
    "        max_files = 1000\n",
    "    \n",
    "    # Step 2: Setup Device\n",
    "    print(\"\\n--- Step 2: Device Setup ---\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    if device.type == 'cuda':\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Step 3: Initialize Model\n",
    "    print(\"\\n--- Step 3: Model Initialization ---\")\n",
    "    model = TinyTransformerCodec().to(device)\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {total_params:,}\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.L1Loss()\n",
    "    spectral_criterion = SpectralLoss().to(device)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Step 4: Load Dataset\n",
    "    print(\"\\n--- Step 4: Loading Dataset ---\")\n",
    "    full_dataset = AudioFileDataset(data_dir=audio_dir, max_files=max_files)\n",
    "    train_size = int(0.9 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Steps per epoch: {len(train_loader)}\")\n",
    "    \n",
    "    # Step 5: Setup Scheduler\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=LEARNING_RATE,\n",
    "        epochs=EPOCHS,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        pct_start=0.1,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "    \n",
    "    start_epoch = 0\n",
    "\n",
    "    # Step 6: Load Checkpoint if exists\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        try:\n",
    "            print(\"\\n--- Loading Checkpoint ---\")\n",
    "            checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "            print(f\"Resumed from epoch {start_epoch}, Best val loss: {best_val_loss:.6f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load checkpoint: {e}\")\n",
    "            print(\"Starting fresh training...\")\n",
    "\n",
    "    # Step 7: Training Loop\n",
    "    print(\"\\n--- Step 5: Starting Training ---\")\n",
    "    print(f\"Training for {EPOCHS} epochs\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(start_epoch, EPOCHS):\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0\n",
    "        reconstruction_loss_accum = 0\n",
    "        spectral_loss_accum = 0\n",
    "        vq_loss_accum = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            audio = batch.to(device)\n",
    "\n",
    "            with torch.amp.autocast(device_type='cuda' if device.type == 'cuda' else 'cpu', dtype=torch.float16):\n",
    "                codes, vq_loss, input_length, _, encoder_outputs = model.encode(audio)\n",
    "                reconstructed_audio = model.decode(codes, input_length, encoder_outputs)\n",
    "\n",
    "                reconstruction_loss = criterion(reconstructed_audio, audio)\n",
    "                spectral_loss = spectral_criterion(reconstructed_audio, audio)\n",
    "                \n",
    "                loss = (reconstruction_loss + SPECTRAL_LOSS_WEIGHT * spectral_loss + VQ_LOSS_WEIGHT * vq_loss) / GRADIENT_ACCUMULATION_STEPS\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "            reconstruction_loss_accum += reconstruction_loss.item()\n",
    "            spectral_loss_accum += spectral_loss.item()\n",
    "            vq_loss_accum += vq_loss.item()\n",
    "            \n",
    "            if (i + 1) % GRADIENT_ACCUMULATION_STEPS == 0 or (i + 1) == len(train_loader):\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIPPING_NORM)\n",
    "                \n",
    "                scaler.step(optimizer)\n",
    "                scheduler.step()\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{total_loss/(i+1):.4f}',\n",
    "                'Recon': f'{reconstruction_loss_accum/(i+1):.4f}',\n",
    "                'Spec': f'{spectral_loss_accum/(i+1):.4f}',\n",
    "                'VQ': f'{vq_loss_accum/(i+1):.4f}'\n",
    "            })\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        print(\"\\nRunning validation...\")\n",
    "        avg_val_loss = validate(model, val_loader, criterion, spectral_criterion, device)\n",
    "        \n",
    "        # Quality Metrics\n",
    "        avg_pesq, avg_stoi = 0.0, 0.0\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            avg_pesq, avg_stoi = run_quality_metrics(model, val_loader, device)\n",
    "        \n",
    "        # Codebook Monitoring\n",
    "        if (epoch + 1) % MONITOR_FREQUENCY == 0:\n",
    "            monitor_codebook_usage(model, val_loader, device)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.6f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.6f}\")\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"  PESQ: {avg_pesq:.4f}, STOI: {avg_stoi:.4f}\")\n",
    "        print(f\"  Time: {epoch_time:.1f}s\")\n",
    "        print(f\"  LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        # Save checkpoint (Local)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': avg_train_loss,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'pesq': avg_pesq,\n",
    "            'stoi': avg_stoi\n",
    "        }, CHECKPOINT_PATH)\n",
    "        \n",
    "        # Save checkpoint (Google Drive)\n",
    "        gdrive_checkpoint_path = os.path.join(GDRIVE_SAVE_DIR, os.path.basename(CHECKPOINT_PATH))\n",
    "        try:\n",
    "            shutil.copyfile(CHECKPOINT_PATH, gdrive_checkpoint_path)\n",
    "            print(f\"  Full checkpoint saved to GDrive: {gdrive_checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Could not save full checkpoint to GDrive: {e}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            \n",
    "            # Save best model (Local)\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'val_loss': best_val_loss,\n",
    "            }, BEST_MODEL_PATH)\n",
    "            print(f\"  *** New best model saved locally! ***\")\n",
    "            \n",
    "            # Save best model (Google Drive)\n",
    "            gdrive_best_model_path = os.path.join(GDRIVE_SAVE_DIR, os.path.basename(BEST_MODEL_PATH))\n",
    "            try:\n",
    "                shutil.copyfile(BEST_MODEL_PATH, gdrive_best_model_path)\n",
    "                print(f\"  *** New best model saved to GDrive: {gdrive_best_model_path} ***\")\n",
    "            except Exception as e:\n",
    "                print(f\"  WARNING: Could not save best model to GDrive: {e}\")\n",
    "\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "    print(f\"Model saved locally to: {BEST_MODEL_PATH}\")\n",
    "    print(f\"Model saved to GDrive folder: {GDRIVE_SAVE_DIR}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# --- Inference Function ---\n",
    "def test_inference(model_path=BEST_MODEL_PATH):\n",
    "    \"\"\"Test the trained model on a sample.\"\"\"\n",
    "    print(\"\\n--- Testing Inference ---\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TinyTransformerCodec().to(device)\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "    else:\n",
    "        print(\"No trained model found. Train first!\")\n",
    "        return\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    test_duration = 2.0\n",
    "    t = torch.linspace(0, test_duration, int(SR * test_duration))\n",
    "    test_audio = torch.sin(2 * np.pi * 440 * t) * 0.5 \n",
    "    test_audio = test_audio.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        codes, _, input_length, indices_list, encoder_outputs = model.encode(test_audio)\n",
    "        reconstructed = model.decode(codes, input_length, encoder_outputs)\n",
    "    \n",
    "    print(f\"Input shape: {test_audio.shape}\")\n",
    "    print(f\"Codes shape: {codes.shape}\")\n",
    "    print(f\"Reconstructed shape: {reconstructed.shape}\")\n",
    "    print(f\"Compression ratio: {test_audio.shape[-1] / codes.shape[-1]:.1f}x\")\n",
    "    \n",
    "    mse = F.mse_loss(reconstructed, test_audio).item()\n",
    "    snr = 10 * np.log10(1.0 / mse) if mse > 0 else float('inf')\n",
    "    print(f\"Reconstruction SNR: {snr:.2f} dB\")\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "if __name__ == '__main__':\n",
    "    print(\"\"\"\n",
    "    ╔══════════════════════════════════════════════════════════╗\n",
    "    ║     TINY TRANSFORMER VQ-CODEC - AUTOMATIC TRAINING       ║\n",
    "    ╠══════════════════════════════════════════════════════════╣\n",
    "    ║  This script will:                                       ║\n",
    "    ║  1. Automatically download the LJSpeech dataset.         ║\n",
    "    ║  2. Train the Tiny Transformer Codec model.              ║\n",
    "    ║  3. Save checkpoints to Google Drive.                    ║\n",
    "    ║                                                          ║\n",
    "    ║  To use a small test dataset, change 'use_mini = False'  ║\n",
    "    ║  to 'use_mini = True' below.                             ║\n",
    "    ╚══════════════════════════════════════════════════════════╝\n",
    "    \"\"\")\n",
    "    \n",
    "    use_mini = False\n",
    "    \n",
    "    train_model(use_mini_dataset=use_mini)\n",
    "    a\n",
    "    # test_inference()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
